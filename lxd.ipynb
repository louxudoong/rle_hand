{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查valid函数的正确性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlepose.trainer import train, validate, validate_gt, validate_gt_3d\n",
    "\n",
    "from rlepose.utils.transforms import get_coord\n",
    "\n",
    "import rlepose.datasets.lxd_freihand as lxd_freihand\n",
    "from rlepose.models import builder\n",
    "from rlepose.utils.config import update_config\n",
    "\n",
    "import torch\n",
    "cfg = update_config(\"./configs/256x192_res50_regress-flow_freihand.yaml\")\n",
    "\n",
    "def preset_model(cfg):\n",
    "    model = builder.build_sppe(cfg.MODEL, preset_cfg=cfg.DATA_PRESET)\n",
    "\n",
    "    if cfg.MODEL.PRETRAINED:\n",
    "        model.load_state_dict(torch.load(cfg.MODEL.PRETRAINED))\n",
    "    elif cfg.MODEL.TRY_LOAD:\n",
    "        pretrained_state = torch.load(cfg.MODEL.TRY_LOAD)\n",
    "        model_state = model.state_dict()\n",
    "        pretrained_state = {k: v for k, v in pretrained_state.items()\n",
    "                            if k in model_state and v.size() == model_state[k].size()}\n",
    "\n",
    "        model_state.update(pretrained_state)\n",
    "        model.load_state_dict(model_state)\n",
    "    else:\n",
    "        model._initialize()\n",
    "\n",
    "    return model\n",
    "\n",
    "m = preset_model(cfg)\n",
    "m.train()\n",
    "\n",
    "hm_shape = cfg.DATA_PRESET.get('HEATMAP_SIZE')\n",
    "depth_dim = cfg.MODEL.get('DEPTH_DIM')\n",
    "output_3d = cfg.DATA_PRESET.get('OUT_3D', False)\n",
    "hm_shape = (hm_shape[1], hm_shape[0], depth_dim)\n",
    "grad_clip = cfg.TRAIN.get('GRAD_CLIP', False)\n",
    "\n",
    "\n",
    "\n",
    "heatmap_to_coord = get_coord(cfg, cfg.DATA_PRESET.HEATMAP_SIZE, output_3d)\n",
    "\n",
    "gt_val_dataset = builder.build_dataset(cfg.DATASET.VAL, preset_cfg=cfg.DATA_PRESET, train=False, heatmap2coord=cfg.TEST.HEATMAP2COORD)\n",
    "gt_val_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    gt_val_dataset, num_replicas=cfg.TRAIN.WORLD_SIZE, rank=1)\n",
    "\n",
    "gt_val_loader = torch.utils.data.DataLoader(\n",
    "    gt_val_dataset, batch_size=32, shuffle=False, num_workers=20, drop_last=False, sampler=gt_val_sampler)\n",
    "\n",
    "\n",
    "# gt_AP = validate_gt(m, opt, cfg, heatmap_to_coord)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inps, labels in gt_val_loader:\n",
    "    bboxes = labels.pop('bbox')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch\n",
    "from rlepose.models import builder\n",
    "from rlepose.utils.valid_utils_lxd import cal_mAP_RMSE\n",
    "import time\n",
    "cfg = update_config(\"./configs/256x192_res50_regress-flow_freihand.yaml\")\n",
    "\n",
    "def paint(im, kpts):\n",
    "    '''\n",
    "    输入为单帧的im与其对应的一组kpts\n",
    "    '''\n",
    "    \n",
    "    colors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0], \\\n",
    "          [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], \\\n",
    "          [128, 0, 128], [255, 192, 203], [0, 128, 128], [128, 128, 0], [128, 0, 0], [0, 128, 0], [0, 0, 128]]\n",
    "    limbSeq = [[0,1],[1,2],[2,3],[3,4],[0,5],[5,6],[6,7],[7,8],[0,9],[9,10],[10,11],[11,12],[0,13],[13,14],[14,15],[15,16],[0,17],[17,18],[18,19],[19,20]]\n",
    "\n",
    "    # im = cv2.imread(img_path)\n",
    "    # draw points\n",
    "    for k in kpts:\n",
    "        x = int(k[0])\n",
    "        y = int(k[1])\n",
    "        # print(f'im .shape = {im.shape}')\n",
    "        # print(f'x={x}, y={y}')\n",
    "        cv2.circle(im, (x, y), 2, (0, 0, 255), -1)\n",
    "\n",
    "    # draw lines\n",
    "    # print(\"kpts shape: \", kpts)\n",
    "    for i in range(len(limbSeq)):\n",
    "        cur_im = im.copy()\n",
    "        limb = limbSeq[i]\n",
    "        # print(kpts[limb[0]])\n",
    "        [Y0, X0] = kpts[limb[0]]\n",
    "        [Y1, X1] = kpts[limb[1]]\n",
    "        mX = np.mean([X0, X1])\n",
    "        mY = np.mean([Y0, Y1])\n",
    "        length = ((X0 - X1) ** 2 + (Y0 - Y1) ** 2) ** 0.5\n",
    "        angle = math.degrees(math.atan2(X0 - X1, Y0 - Y1))\n",
    "        polygon = cv2.ellipse2Poly((int(mY), int(mX)), (int(length / 2), 4), int(angle), 0, 360, 1)\n",
    "        cv2.fillConvexPoly(cur_im, polygon, colors[i])\n",
    "        im = cv2.addWeighted(im, 0.4, cur_im, 0.6, 0)\n",
    "\n",
    "    return im\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "m = builder.build_sppe(cfg.MODEL, preset_cfg=cfg.DATA_PRESET)  # 根据cfg的配置信息构建模型\n",
    "\n",
    "m.load_state_dict(torch.load(\"/home/louxd/ldlib/res-loglikelihood-regression-master/weights/model_114.pth\", map_location='cpu'), \n",
    "                  strict=True)  # 加载权重\n",
    "\n",
    "m.cuda(device)  # 把模型放到gpu中\n",
    "\n",
    "time_cost_list = []\n",
    "with torch.no_grad():\n",
    "    gt_val_dataset = builder.build_dataset(cfg.DATASET.VAL, preset_cfg=cfg.DATA_PRESET, train=False, heatmap2coord=cfg.TEST.HEATMAP2COORD)\n",
    "    gt_val_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        gt_val_dataset, num_replicas=1, rank=0)\n",
    "\n",
    "    gt_val_loader = torch.utils.data.DataLoader(\n",
    "        gt_val_dataset, batch_size=16, shuffle=False, num_workers=20, drop_last=False, sampler=gt_val_sampler)\n",
    "    \n",
    "    \n",
    "    mAP_ary = []\n",
    "    RMSE_ary = []\n",
    "    for index, (inps, labels) in enumerate(gt_val_loader):\n",
    "        inps = inps.cuda(device)\n",
    "\n",
    "        for k, _ in labels.items():\n",
    "            if k == 'type':\n",
    "                continue\n",
    "            \n",
    "            labels[k] = labels[k].cuda(device)\n",
    "\n",
    "        reason_start_time = time.time()  # 记录开始时间\n",
    "        output = m(inps, labels)\n",
    "        reaon_end_time = time.time()  # 记录结束时间\n",
    "        reason_cost_time = reaon_end_time - reason_start_time  # 计算耗时，单位为秒\n",
    "        time_cost_list.append(reason_cost_time)\n",
    "\n",
    "        if isinstance(inps, list):\n",
    "            batch_size = inps[0].size(0)\n",
    "        else:\n",
    "            batch_size = inps.size(0)\n",
    "\n",
    "        kpts_gt = labels['target_uv'].cpu().numpy().reshape(-1, 21, 2) # size * 42\n",
    "        kpts_pre = output.pred_jts.cpu().numpy().reshape(-1, 21, 2)\n",
    "\n",
    "        mAP_str, RMSE = cal_mAP_RMSE(kpts_pre, kpts_gt)\n",
    "        \n",
    "        # print & draw\n",
    "        for i in range(len(inps)):\n",
    "            if i == 0 & index < 100:\n",
    "                #print(f'batch:{batch_idx}_{i}')\n",
    "                imgi = inps[i].cpu().numpy()\n",
    "                imgi = np.transpose(imgi, (1, 2, 0))\n",
    "                imgi = (imgi + np.array([0.480, 0.457, 0.406], dtype=np.float32)) * np.array([255., 255., 255.], dtype=np.float32)\n",
    "                imgi = cv2.cvtColor(imgi, cv2.COLOR_BGR2RGB) \n",
    "                img_h, img_w, _ = imgi.shape\n",
    "                kpts_pre_i = np.array([(kpt + [0.5, 0.5]) * [img_w, img_h] for kpt in kpts_pre[i]])\n",
    "                kpts_gt_i = np.array([(kpt + [0.5, 0.5]) * [img_w, img_h] for kpt in kpts_gt[i]])\n",
    "\n",
    "                imagei_pre = imgi.copy()\n",
    "                imagei_gt = imgi.copy()\n",
    "                imagei_pre = paint(imagei_pre, kpts_pre_i)\n",
    "                imagei_gt = paint(imagei_gt, kpts_gt_i)\n",
    "                cv2.imwrite(f'./exp/output_114/{index}_{i}_pre.jpg', imagei_pre)\n",
    "                cv2.imwrite(f'./exp/output_114/{index}_{i}_gt.jpg', imagei_gt)\n",
    "\n",
    "avg_pre_cost = sum(time_cost_list) / len(time_cost_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_pre_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试mAP计算模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from rlepose.models import builder\n",
    "from rlepose.utils.config import update_config\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "cfg = update_config(\"./configs/256x192_res50_regress-flow_freihand.yaml\")\n",
    "device = torch.device('cuda')\n",
    "m = builder.build_sppe(cfg.MODEL, preset_cfg=cfg.DATA_PRESET)  # 根据cfg的配置信息构建模型\n",
    "\n",
    "m.load_state_dict(torch.load(\"/home/louxd/ldlib/res-loglikelihood-regression-master/weights/model_114.pth\", map_location='cpu'), \n",
    "                  strict=True)  # 加载权重\n",
    "\n",
    "m.cuda(device)  # 把模型放到gpu中\n",
    "\n",
    "with torch.no_grad():\n",
    "    gt_val_dataset = builder.build_dataset(cfg.DATASET.VAL, preset_cfg=cfg.DATA_PRESET, train=False, heatmap2coord=cfg.TEST.HEATMAP2COORD)\n",
    "    gt_val_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        gt_val_dataset, num_replicas=1, rank=0)\n",
    "\n",
    "    gt_val_loader = torch.utils.data.DataLoader(\n",
    "        gt_val_dataset, batch_size=16, shuffle=False, num_workers=20, drop_last=False, sampler=gt_val_sampler)\n",
    "    \n",
    "    \n",
    "    rmse_list = []\n",
    "    oks_list = []\n",
    "    time_cost_list = []\n",
    "\n",
    "    from rlepose.utils.valid_utils_lxd import compute_RMSE, calculate_oks_pt2, paint, calculate_mAP\n",
    "\n",
    "    for index, (inps, labels) in enumerate(gt_val_loader):\n",
    "        inps = inps.cuda(device)\n",
    "        for k, _ in labels.items():\n",
    "            if k == 'type':\n",
    "                continue\n",
    "            \n",
    "            labels[k] = labels[k].cuda(device)\n",
    "\n",
    "        # output = m(inps, labels)\n",
    "        output = m(inps)\n",
    "\n",
    "        if isinstance(inps, list):\n",
    "            batch_size = inps[0].size(0)\n",
    "        else:\n",
    "            batch_size = inps.size(0)\n",
    "\n",
    "        kpts_gt = labels['target_uv'].cpu().numpy().reshape(-1, 21, 2) # size * 42\n",
    "        kpts_pre = output.pred_jts.cpu().numpy().reshape(-1, 21, 2)\n",
    "\n",
    "        \n",
    "        # print & draw\n",
    "        for i in range(len(inps)):\n",
    "            #print(f'batch:{batch_idx}_{i}')\n",
    "            imgi = inps[i].cpu().numpy()\n",
    "            imgi = np.transpose(imgi, (1, 2, 0))\n",
    "            imgi = (imgi + np.array([0.480, 0.457, 0.406], dtype=np.float32)) * np.array([255., 255., 255.], dtype=np.float32)\n",
    "            imgi = cv2.cvtColor(imgi, cv2.COLOR_BGR2RGB) \n",
    "            img_h, img_w, _ = imgi.shape\n",
    "            kpts_pre_i = np.array([(kpt + [0.5, 0.5]) * [img_w, img_h] for kpt in kpts_pre[i]])\n",
    "            kpts_gt_i = np.array([(kpt + [0.5, 0.5]) * [img_w, img_h] for kpt in kpts_gt[i]])\n",
    "\n",
    "            # cal mAP and rmse\n",
    "            # rmsei = compute_RMSE(kpts_pre_i, kpts_gt_i)\n",
    "            msei = mean_squared_error(kpts_pre_i, kpts_gt_i)\n",
    "            rmsei = np.sqrt(msei)\n",
    "            oksi = calculate_oks_pt2(kpts_pre_i, kpts_gt_i)\n",
    "            rmse_list.append(rmsei)\n",
    "            oks_list.append(oksi)\n",
    "\n",
    "            imagei_pre = imgi.copy()\n",
    "            imagei_gt = imgi.copy()\n",
    "            imagei_pre = paint(imagei_pre, kpts_pre_i)\n",
    "            imagei_gt = paint(imagei_gt, kpts_gt_i)\n",
    "            cv2.imwrite(f'./exp/output/{index}_{i}_pre.jpg', imagei_pre)\n",
    "            cv2.imwrite(f'./exp/output/{index}_{i}_gt.jpg', imagei_gt)\n",
    "\n",
    "\n",
    "    rmse = sum(rmse_list) / len(rmse_list)\n",
    "    mAP_info_str = calculate_mAP(oks_list)\n",
    "\n",
    "    ap = mAP_info_str['mAP']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = cfg.DATA_PRESET['IMAGE_SIZE']\n",
    "output_size = cfg.DATA_PRESET['HEATMAP_SIZE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试on camera的图像归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000000.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000001.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000002.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000003.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000004.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000005.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000006.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000007.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000008.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000009.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000010.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000011.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000012.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000013.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000014.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000015.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000016.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000017.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000018.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000019.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000020.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000021.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000022.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000023.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000024.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000025.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000026.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000027.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000028.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000029.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000030.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000031.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000032.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000033.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000034.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000035.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000036.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000037.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000038.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000039.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000040.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000041.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000042.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000043.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000044.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000045.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000046.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000047.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000048.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000049.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000050.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000051.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000052.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000053.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000054.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000055.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000056.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000057.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000058.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000059.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000060.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000061.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000062.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000063.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000064.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000065.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000066.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000067.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000068.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000069.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000070.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000071.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000072.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000073.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000074.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000075.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000076.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000077.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000078.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000079.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000080.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000081.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000082.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000083.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000084.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000085.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000086.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000087.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000088.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000089.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000090.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000091.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000092.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000093.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000094.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000095.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000096.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000097.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000098.jpg\n",
      "/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000099.jpg\n",
      "anno_xyz len： 3960\n",
      "anno_K len： 3960\n",
      "img list len： 3960\n",
      "AUG OFF.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from rlepose.models import builder\n",
    "from rlepose.utils.config import update_config\n",
    "from rlepose.utils.valid_utils_lxd import paint\n",
    "cfg = update_config(\"./configs/256x192_res50_regress-flow_freihand.yaml\")\n",
    "from rlepose.utils.transforms import im_to_torch\n",
    "\n",
    "device = torch.device('cuda')\n",
    "m = builder.build_sppe(cfg.MODEL, preset_cfg=cfg.DATA_PRESET)  # 根据cfg的配置信息构建模型\n",
    "\n",
    "m.load_state_dict(torch.load(\"/home/louxd/ldlib/res-loglikelihood-regression-master/weights/model_114.pth\", map_location='cpu'), \n",
    "                  strict=True)  # 加载权重\n",
    "\n",
    "m.cuda(device)  # 把模型放到gpu中\n",
    "\n",
    "input_size = cfg.DATA_PRESET['IMAGE_SIZE']\n",
    "output_size = cfg.DATA_PRESET['HEATMAP_SIZE']\n",
    "\n",
    "with torch.no_grad():\n",
    "    # gt_val_dataset = builder.build_dataset(cfg.DATASET.VAL, preset_cfg=cfg.DATA_PRESET, train=False, heatmap2coord=cfg.TEST.HEATMAP2COORD)\n",
    "    # gt_val_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    #     gt_val_dataset, num_replicas=1, rank=0)\n",
    "\n",
    "    # gt_val_loader = torch.utils.data.DataLoader(\n",
    "    #     gt_val_dataset, batch_size=16, shuffle=False, num_workers=20, drop_last=False, sampler=gt_val_sampler)\n",
    "    dir = \"/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/\"\n",
    "    img_format = \".jpg\"\n",
    "\n",
    "    for j in range(0, 1000):\n",
    "        img_name = f\"{j:08d}{img_format}\"\n",
    "        # 拼接完整的文件路径\n",
    "        img_path = dir + img_name\n",
    "        # print(img_path)\n",
    "        # 读取图片\n",
    "        init_img = cv2.imread(img_path)\n",
    "\n",
    "        # init_img = cv2.imread(\"/home/louxd/dataset/FreiHand/FreiHAND_pub_v2/evaluation/rgb/00000006.jpg\")\n",
    "        src = cv2.cvtColor(init_img, cv2.COLOR_BGR2RGB)\n",
    "        height, width = src.shape[0], src.shape[1]\n",
    "        label = {\n",
    "        # 'bbox': (xmin, ymin, xmax, ymax),\n",
    "        'bbox': (1, 1, 2, 2), \n",
    "        'width': width,\n",
    "        'height': height,\n",
    "        'joints_3d': 0\n",
    "        }\n",
    "\n",
    "        input_size = cfg.DATA_PRESET['IMAGE_SIZE']\n",
    "        heatmap_size = cfg.DATA_PRESET['HEATMAP_SIZE']\n",
    "        \n",
    "        imgwidth, imght = label['width'], label['height']\n",
    "        assert imgwidth == src.shape[1] and imght == src.shape[0]\n",
    "\n",
    "        inp_h, inp_w = input_size\n",
    "        img = cv2.resize(src, (int(inp_w), int(inp_h)))\n",
    "        img = im_to_torch(img)\n",
    "        img[0].add_(-0.406)\n",
    "        img[1].add_(-0.457)\n",
    "        img[2].add_(-0.480)\n",
    "\n",
    "        inps = torch.from_numpy(np.expand_dims(img, axis=0))\n",
    "        inps = inps.cuda(device)\n",
    "        output = m(inps)\n",
    "\n",
    "        kpts_pre = output.pred_jts.cpu().numpy().reshape(-1, 21, 2)\n",
    "\n",
    "        \n",
    "\n",
    "        for i in range(len(inps)):\n",
    "            imgi = inps[i].cpu().numpy()\n",
    "            imgi = np.transpose(imgi, (1, 2, 0))\n",
    "            imgi = (imgi + np.array([0.480, 0.457, 0.406], dtype=np.float32))  * np.array([255., 255., 255.], dtype=np.float32)\n",
    "            imgi = cv2.cvtColor(imgi, cv2.COLOR_BGR2RGB) \n",
    "            img_h, img_w, _ = imgi.shape\n",
    "            imagei_pre = imgi.copy()\n",
    "            kpts_pre_i = np.array([(kpt + [0.5, 0.5]) * [img_w, img_h] for kpt in kpts_pre[i]])\n",
    "            imagei_pre = paint(imagei_pre, kpts_pre_i)\n",
    "            # cv2.imshow(\"1\", imagei_pre)\n",
    "            # cv2.imshow(\"0\", init_img)\n",
    "            # cv2.waitKey(0)\n",
    "            cv2.imwrite(f'./exp/output/{j}.jpg', imagei_pre)\n",
    "            # cv2.imwrite(f'./exp/out/init.jpg', init_img)\n",
    "    \n",
    "\n",
    "    gt_val_dataset = builder.build_dataset(cfg.DATASET.VAL, preset_cfg=cfg.DATA_PRESET, train=False, heatmap2coord=cfg.TEST.HEATMAP2COORD)\n",
    "    gt_val_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        gt_val_dataset, num_replicas=1, rank=0)\n",
    "\n",
    "    gt_val_loader = torch.utils.data.DataLoader(\n",
    "        gt_val_dataset, batch_size=16, shuffle=False, num_workers=20, drop_last=False, sampler=gt_val_sampler)\n",
    "    \n",
    "    \n",
    "    rmse_list = []\n",
    "    oks_list = []\n",
    "    time_cost_list = []\n",
    "\n",
    "    from rlepose.utils.valid_utils_lxd import compute_RMSE, calculate_oks_pt2, paint, calculate_mAP\n",
    "\n",
    "    for index, (inps, labels) in enumerate(gt_val_loader):\n",
    "        inps = inps.cuda(device)\n",
    "        for k, _ in labels.items():\n",
    "            if k == 'type':\n",
    "                continue\n",
    "            \n",
    "            labels[k] = labels[k].cuda(device)\n",
    "\n",
    "        # output = m(inps, labels)\n",
    "        output = m(inps)\n",
    "\n",
    "        if isinstance(inps, list):\n",
    "            batch_size = inps[0].size(0)\n",
    "        else:\n",
    "            batch_size = inps.size(0)\n",
    "\n",
    "        kpts_gt = labels['target_uv'].cpu().numpy().reshape(-1, 21, 2) # size * 42\n",
    "        kpts_pre = output.pred_jts.cpu().numpy().reshape(-1, 21, 2)\n",
    "\n",
    "        \n",
    "        # print & draw\n",
    "        for i in range(len(inps)):\n",
    "            #print(f'batch:{batch_idx}_{i}')\n",
    "            imgi = inps[i].cpu().numpy()\n",
    "            imgi = np.transpose(imgi, (1, 2, 0))\n",
    "            imgi = (imgi + np.array([0.480, 0.457, 0.406], dtype=np.float32)) * np.array([255., 255., 255.], dtype=np.float32)\n",
    "            imgi = cv2.cvtColor(imgi, cv2.COLOR_BGR2RGB) \n",
    "            img_h, img_w, _ = imgi.shape\n",
    "            kpts_pre_i = np.array([(kpt + [0.5, 0.5]) * [img_w, img_h] for kpt in kpts_pre[i]])\n",
    "            kpts_gt_i = np.array([(kpt + [0.5, 0.5]) * [img_w, img_h] for kpt in kpts_gt[i]])\n",
    "\n",
    "\n",
    "            imagei_pre = imgi.copy()\n",
    "            imagei_gt = imgi.copy()\n",
    "            imagei_pre = paint(imagei_pre, kpts_pre_i)\n",
    "            imagei_gt = paint(imagei_gt, kpts_gt_i)\n",
    "            cv2.imwrite(f'./exp/output_1/{16 * index + i}_{i}_pre.jpg', imagei_pre)\n",
    "            # cv2.imwrite(f'./exp/output_1/{index}_{i}_gt.jpg', imagei_gt)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "thresholds = np.arange(0.5, 1., 0.05)\n",
    "num_thresholds = len(thresholds)\n",
    "average_precision = mAP_info_str.values()\n",
    "\n",
    "plt.plot(thresholds, average_precision, '-o')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Average Precision')\n",
    "plt.title('mAP Curve')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
